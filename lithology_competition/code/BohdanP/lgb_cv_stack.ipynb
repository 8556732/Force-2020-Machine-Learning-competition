{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script by Bohdan Pavlyshenko, b.pavlyshenko@gmail.com, https://www.linkedin.com/in/bpavlyshenko/\n",
    "#\n",
    "# Some code in this script was taken from the starter jupyter notebook from \n",
    "# competition 'FORCE: Machine Predicted Lithology' (https://xeek.ai/challenges/force-well-logs/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_true, y_pred):\n",
    "    S = 0.0\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        S -= A[y_true[i], y_pred[i]]\n",
    "    return S/y_true.shape[0]\n",
    "def feval_f(y_pred, dset):\n",
    "    y_true = dset.get_label()\n",
    "    y_pred1=y_pred.reshape(12,-1)\n",
    "    y_v=np.argmax(y_pred1,axis=0)\n",
    "    score_v=-score(y_true, y_v)\n",
    "    return('score', score_v, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up options\n",
    "n_splits=10\n",
    "test_size=0.15\n",
    "# directory for models and model predictions\n",
    "model_dir='models/'\n",
    "file_dir='stacking/'\n",
    "# load penalty matrix \n",
    "A = np.load('data/penalty_matrix.npy')\n",
    "# load train and test sets\n",
    "data_train = pd.read_csv('data/train.csv', sep=';')\n",
    "data_test=pd.read_csv('data/test.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ntrain=data_train.shape[0]\n",
    "data=pd.concat([data_train,data_test],axis=0)\n",
    "\n",
    "enc=LabelEncoder()\n",
    "data.FORMATION=enc.fit_transform(data.FORMATION.astype(str))\n",
    "pickle.dump(enc, open(model_dir+'formation_enc.pkl', 'wb'))\n",
    "\n",
    "enc=LabelEncoder()\n",
    "data.GROUP=enc.fit_transform(data.GROUP.astype(str))\n",
    "pickle.dump(enc, open(model_dir+'group_enc.pkl', 'wb'))\n",
    "\n",
    "data_train=data.iloc[:ntrain,:].copy()\n",
    "data_test=data.iloc[ntrain:,:].copy()\n",
    "data_train['y'] = data_train['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "\n",
    "# This part of the code is based on the code from the starter jupyter notebook \n",
    "lithology_numbers = {30000: 0,\n",
    "                 65030: 1,\n",
    "                 65000: 2,\n",
    "                 80000: 3,\n",
    "                 74000: 4,\n",
    "                 70000: 5,\n",
    "                 70032: 6,\n",
    "                 88000: 7,\n",
    "                 86000: 8,\n",
    "                 99000: 9,\n",
    "                 90000: 10,\n",
    "                 93000: 11}\n",
    "data_train['y'] = data_train['y'].map(lithology_numbers)\n",
    "######################################################################\n",
    "\n",
    "features_set1=['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'GROUP', 'FORMATION',\n",
    "       'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR','SGR', 'NPHI', 'PEF',\n",
    "       'DTC', 'SP', 'BS', 'ROP', 'DTS', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC',\n",
    "       'ROPA', 'RXO']\n",
    "\n",
    "features_set2=['Z_LOC', 'GROUP', 'FORMATION',\n",
    "       'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'SGR', 'NPHI', 'PEF',\n",
    "       'DTC', 'SP', 'BS', 'ROP', 'DTS', 'DCAL', 'DRHO', 'MUDWEIGHT', 'RMIC',\n",
    "       'ROPA', 'RXO']\n",
    "\n",
    "par_list={\n",
    "'par1':{\n",
    "'features':features_set1,\n",
    "'parameters':  {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 25,\n",
    "    'feature_fraction': 0.15,\n",
    "    'bagging_fraction': 0.05,\n",
    "    'learning_rate': 0.1,\n",
    "     'max_depth':5,\n",
    "    \"lambda_l2\" : 3,\n",
    "    'num_iterations':100,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}\n",
    "},\n",
    "\n",
    "'par2':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 300,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':7,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'metric':'None',\n",
    "    'verbose':-1,\n",
    "    'num_iterations':450,\n",
    "}},\n",
    "\n",
    "\n",
    "'par3':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 25,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.15,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':7,\n",
    "    \"lambda_l2\" : 10,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'min_data_in_leaf': 150, \n",
    "    'bagging_freq': 30,\n",
    "    'num_iterations':650,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}},\n",
    "    \n",
    "'par4':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 500,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.01,\n",
    "    'learning_rate': 0.015,\n",
    "    'max_depth':7,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'metric':'None',\n",
    "    'verbose':-1,\n",
    "    'num_iterations':300,\n",
    "}},\n",
    "\n",
    "'par5':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 250,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.05,\n",
    "    'learning_rate': 0.015,\n",
    "    'max_depth':7,\n",
    "    \"lambda_l2\" : 10,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'min_data_in_leaf': 150, \n",
    "    'bagging_freq': 30,\n",
    "    'num_iterations':250,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}},\n",
    "  \n",
    "'par6':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 500,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.15,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':5,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'min_data_in_leaf': 150, \n",
    "    'bagging_freq': 30,\n",
    "    'num_iterations':550,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}},\n",
    "    \n",
    "'par7':{\n",
    "'features':features_set1,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 100,\n",
    "    'feature_fraction': 0.25,\n",
    "    'bagging_fraction': 0.05,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':-1,\n",
    "    \"lambda_l2\" : 1,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'num_iterations':550,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}},\n",
    "\n",
    "    'par8':{\n",
    "'features':features_set2,\n",
    "'parameters':{\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':12,\n",
    "    'metric': 'multi_logloss',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 100,\n",
    "    'feature_fraction': 0.35,\n",
    "    'bagging_fraction': 0.1,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':7,\n",
    "    \"bagging_seed\" : 15,\n",
    "     \"seed\": 15,\n",
    "    'num_iterations':500,\n",
    "    'metric':'None',\n",
    "    'verbose':-1\n",
    "}}\n",
    "}\n",
    "\n",
    "for i_par in par_list.keys():\n",
    "    print('\\nparameters set',i_par )\n",
    "    features=par_list[i_par]['features']\n",
    "    parameters=par_list[i_par]['parameters'].copy()\n",
    "    print('cross-validation')\n",
    "    gfld =GroupShuffleSplit(n_splits=n_splits,test_size=test_size, random_state=15)\n",
    "    train_d = lgb.Dataset(data_train[features], label=data_train.y,\n",
    "                                  categorical_feature=['FORMATION','GROUP'],free_raw_data=False)\n",
    "    model=lgb.cv(parameters, train_d,  folds=gfld.split(data_train, data_train.y, data_train.WELL),\\\n",
    "                 return_cvbooster=True)\n",
    "    boosters=model['cvbooster'].boosters\n",
    "    i=0\n",
    "    pred_list=[]\n",
    "    y_arr_list=[]\n",
    "    save_y=0\n",
    "    gfld =GroupShuffleSplit(n_splits=n_splits,test_size=test_size, random_state=15)\n",
    "    for train_indx, test_indx in gfld.split(data_train, data_train.y, data_train.WELL):\n",
    "        pred=boosters[i].predict(data_train.iloc[test_indx][features])\n",
    "        i=i+1\n",
    "        y_arr_list.append(data_train.iloc[test_indx][['y']].values)\n",
    "        pred_list.append(pred)\n",
    "    predcv=np.vstack(pred_list)\n",
    "    if (save_y==0):\n",
    "        save_y=1\n",
    "        ycv=np.vstack(y_arr_list)\n",
    "        pd.DataFrame(ycv,columns=['y']).to_csv(file_dir+'y.csv',index=False)\n",
    "    pd.DataFrame(predcv.round(5)).to_csv(file_dir+i_par+'.csv', index=False)\n",
    "    cv_pred_res=np.argmax(predcv,axis=1)\n",
    "    score_cv=score(ycv,cv_pred_res)\n",
    "    print ('score',score_cv)\n",
    "    print ('test prediction')\n",
    "    train_dt=lgb.Dataset(data_train[features], label=data_train.y,categorical_feature=['FORMATION','GROUP'])\n",
    "    model = lgb.train(parameters,train_dt)\n",
    "    pickle.dump(model, open(model_dir+'lgb_model_'+str(i_par)+'.pkl', 'wb'))\n",
    "    test_pred_p = model.predict(data_test[features])\n",
    "    pd.DataFrame(test_pred_p.round(5)).to_csv(file_dir+i_par+'_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stack_list=list(par_list.keys())\n",
    "stack_list1=['par1', 'par2', 'par3', 'par4', 'par5', 'par6', 'par7', 'par8']\n",
    "n_stack_list=len(stack_list)\n",
    "y_df=pd.read_csv(file_dir+'y.csv')\n",
    "stack_train_df=[]\n",
    "stack_test_df=[]\n",
    "for i_par in stack_list:\n",
    "    trn_st=pd.read_csv(file_dir+i_par+'.csv')\n",
    "    stack_train_df.append(trn_st)\n",
    "    tst_st=pd.read_csv(file_dir+i_par+'_test.csv')\n",
    "    stack_test_df.append(tst_st)\n",
    "    \n",
    "X=np.hstack(stack_train_df)\n",
    "Xtest=np.hstack(stack_test_df)\n",
    "ntrn=X.shape[0]\n",
    "ntest=Xtest.shape[0]\n",
    "\n",
    "test_res_p=np.zeros([ntest,12])\n",
    "trn_res_p=np.zeros([ntrn,12])\n",
    "for i in np.arange(12):\n",
    "    print ('class',i)\n",
    "    y=np.zeros(ntrn)\n",
    "    y[y_df.y==i]=1\n",
    "    lr=LogisticRegression(n_jobs=12)\n",
    "    lr.fit(X,y)\n",
    "    pickle.dump(lr, open(model_dir+'lr_model_'+str(i)+'.pkl', 'wb'))\n",
    "    test_res_p[:,i]=lr.predict_proba(Xtest)[:,1]\n",
    "    trn_res_p[:,i]=lr.predict_proba(X)[:,1]\n",
    "\n",
    "trn_pred=np.argmax(trn_res_p,axis=1)\n",
    "cv_score=score(y_df.y.values,trn_pred)\n",
    "print(f'cv_score:{cv_score}')\n",
    "\n",
    "test_pred=np.argmax(test_res_p,axis=1)\n",
    "\n",
    "# This part of the code is from the starter jupyter notebook \n",
    "category_to_lithology = {y:x for x,y in lithology_numbers.items()}\n",
    "test_prediction_for_submission = np.vectorize(category_to_lithology.get)(test_pred)\n",
    "np.savetxt('prediction.csv', test_prediction_for_submission, header='lithology', comments='', fmt='%i')\n",
    "################################################################################\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
