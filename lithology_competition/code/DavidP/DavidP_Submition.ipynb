{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import joblib\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.targets = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        # Load pre-trained model from file\n",
    "#         self.model = pickle.load(open(model_file, 'rb'))\n",
    "        # Load a \"pre-trained\" scaler from file\n",
    "#         self.scaler = pickle.load(open(scaler_file, 'rb'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _check_depth_step_(df, step):\n",
    "        residual = (df['Depth'] - df.groupby('Well')['Depth'].shift()).fillna(step)\n",
    "        residual = residual - step\n",
    "        msk = (residual != 0)\n",
    "        print('## Number of incorrect depth samples: ', msk.sum())\n",
    "        outlier_wells = df.loc[msk]['Well'].unique()\n",
    "        print('## Wells with incorrect depth step: ', outlier_wells)\n",
    "        \n",
    "    def _preprocess_raw_(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        print('# Drop & rename')\n",
    "        df = df.copy()\n",
    "        df.drop(columns=['SGR'], inplace=True)  # not in test\n",
    "        df.rename(columns={\n",
    "                    'WELL':'Well', \n",
    "                    'DEPTH_MD':'Depth', \n",
    "                        }, inplace=True)\n",
    "\n",
    "        df['Depth'] = 1000 * df['Depth']\n",
    "        df['Depth'] = df['Depth'].round().astype(int)\n",
    "        df = df.sort_values(by=['Well', 'Depth'])\n",
    "        \n",
    "        print('# Check dublicates:')\n",
    "        if df.duplicated(subset=['Well', 'Depth'], keep=False).any(axis=0):\n",
    "            raise Exception('Data containes dublicates')\n",
    "        else:\n",
    "            print('Not Found')\n",
    "\n",
    "        step = 152\n",
    "        self._check_depth_step_(df, step)\n",
    "        \n",
    "        print('# Apply Rebin')\n",
    "        def rebin(df_well, step=step):\n",
    "            start = df_well['Depth'].min()\n",
    "            end = df_well['Depth'].max()\n",
    "            num = (end - start) / step + 1\n",
    "            depthes = np.linspace(start, end, round(num), dtype=np.int)\n",
    "            return depthes\n",
    "\n",
    "        rebined_raw = df.groupby('Well').apply(rebin)\n",
    "        rebined_raw = rebined_raw.to_frame('Depth')\n",
    "        df_base = rebined_raw.explode('Depth')\n",
    "        df_base['Depth'] = df_base['Depth'].astype(int)\n",
    "        df_base.reset_index(inplace=True)\n",
    "        df_m = df_base.merge(df, how='outer', on=['Well', 'Depth'])\n",
    "\n",
    "        print('# Fill missing values')\n",
    "        df_m['GR'] = df_m.groupby('Well')[['GR']].apply(lambda group: group.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "        print('# Fill nan values')\n",
    "        df_m['Sector'] = df_m['Well'].apply(lambda x: x.split('/')[0])\n",
    "        df_m['Sector'] = df_m['Sector'].astype(int)\n",
    "        \n",
    "        df_m['GROUP'] = df_m['GROUP'].fillna('Unknown')\n",
    "        df_m['GROUP'] = df_m['GROUP'].astype(str)\n",
    "        \n",
    "        df_m['FORMATION'] = df_m['FORMATION'].fillna('Unknown')\n",
    "        df_m['FORMATION'] = df_m['FORMATION'].astype(str)\n",
    "        \n",
    "        print('# Set Index')\n",
    "        df_m = df_m.set_index(keys=['Well', 'Depth'], verify_integrity=True)\n",
    "        return df_m    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _make_features_(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        preprocess_features = ['GR',  'X_LOC', 'Y_LOC', 'Z_LOC',\n",
    "                    'CALI', 'NPHI', 'RMED', 'RDEP', 'SP', \n",
    "                    'DTC', 'RHOB', 'BS', 'DTS', 'MUDWEIGHT', 'DCAL']\n",
    "\n",
    "        lag_space = np.ceil(np.logspace(0, 10, num=10, base=2, endpoint=False)).astype(int)\n",
    "        window_space = np.ceil(np.logspace(2, 5, num=8, base=3, endpoint=True)).astype(int)\n",
    "        min_max_space = np.ceil(np.logspace(2, 5, num=8, base=3, endpoint=True)).astype(int)\n",
    "        std_space = np.ceil(np.logspace(2, 5, num=8, base=3, endpoint=True)).astype(int)\n",
    "        diffs_space = np.ceil(np.logspace(0, 10, num=10, base=2, endpoint=False)).astype(int)\n",
    "        gradient_space = np.linspace(1, 6, num=6).astype(int)\n",
    "\n",
    "        def _create_lag_tao_i(data, keys, i):\n",
    "            df_lag_tao_i = data.groupby(['Well'])[keys].apply(lambda series: series.shift(i))\n",
    "            df_lag_tao_i = df_lag_tao_i.rename(columns={key: key + f'_Lag_Tao+{i}' for key in keys})\n",
    "            return df_lag_tao_i\n",
    "\n",
    "        def _create_rolling_mean(data, keys, window, tao):\n",
    "            df_rolling_mean = data.groupby(['Well'])[keys].apply(lambda series: series.shift(tao).rolling(window=window, min_periods=2).mean())\n",
    "            df_rolling_mean = df_rolling_mean.rename(columns={key: key + f'_Rolling_Mean_Shift={tao}_Window={window}' for key in keys})\n",
    "            return df_rolling_mean\n",
    "\n",
    "        def _create_rolling_std(data, keys, window, tao):\n",
    "            df_rolling_std = data.groupby(['Well'])[keys].apply(lambda series: series.shift(tao).rolling(window=window, min_periods=2).std())\n",
    "            df_rolling_std = df_rolling_std.rename(columns={key: key + f'_Rolling_STD_Shift={tao}_Window={window}' for key in keys})\n",
    "            return df_rolling_std\n",
    "\n",
    "        def _create_rolling_max(data, keys, window, tao):\n",
    "            df_rolling_mean = data.groupby(['Well'])[keys].apply(lambda series: series.shift(tao).rolling(window=window, min_periods=2).max())\n",
    "            df_rolling_mean = df_rolling_mean.rename(columns={key: key + f'_Rolling_Max_Shift={tao}_Window={window}' for key in keys})\n",
    "            return df_rolling_mean\n",
    "\n",
    "        def _create_rolling_min(data, keys, window, tao):\n",
    "            df_rolling_mean = data.groupby(['Well'])[keys].apply(lambda series: series.shift(tao).rolling(window=window, min_periods=2).min())\n",
    "            df_rolling_mean = df_rolling_mean.rename(columns={key: key + f'_Rolling_Min_Shift={tao}_Window={window}' for key in keys})\n",
    "            return df_rolling_mean\n",
    "\n",
    "        def _create_rolling_diff(data, keys, periods):\n",
    "            df_rolling_diff = data.groupby(['Well'])[keys].apply(lambda series: series.diff(periods=periods))\n",
    "            df_rolling_diff = df_rolling_diff.rename(columns={key: key + f'_Rolling_Diff_Period={periods}' for key in keys})\n",
    "            return df_rolling_diff\n",
    "\n",
    "        def _create_gradient(data, keys, period, step):\n",
    "            grid = data.index.to_frame()['Depth'].groupby('Well').diff(period).fillna(step)\n",
    "            df_rolling_diff = data.groupby(['Well'])[keys].apply(lambda series: series.diff(period))\n",
    "            df_grad = df_rolling_diff.div(grid, axis=0)\n",
    "            df_grad = df_grad.rename(columns={key: key + f'_Gradient_Period={period}' for key in keys})\n",
    "            return df_grad\n",
    "\n",
    "        data_frames = []\n",
    "\n",
    "        print('# Lags forward')\n",
    "        for i in lag_space:\n",
    "            data_frames.append(_create_lag_tao_i(df, preprocess_features, i))\n",
    "\n",
    "        print('# Lags inverse backward')\n",
    "        for i in lag_space:\n",
    "            data_frames.append(_create_lag_tao_i(df, preprocess_features, -i))\n",
    "\n",
    "        print('# Rolling mean centered')\n",
    "        for i in window_space:\n",
    "            data_frames.append(_create_rolling_mean(df, preprocess_features, i, int(round((i - 1)/2))))\n",
    "\n",
    "        print('# Rolling max centered')\n",
    "        for i in min_max_space:\n",
    "            data_frames.append(_create_rolling_max(df, preprocess_features, i, int(round((i - 1)/2))))\n",
    "\n",
    "        print('# Rolling min centered')\n",
    "        for i in min_max_space:\n",
    "            data_frames.append(_create_rolling_min(df, preprocess_features, i, int(round((i - 1)/2))))\n",
    "\n",
    "        print('# Rolling std centered') \n",
    "        for i in std_space:\n",
    "            data_frames.append(_create_rolling_std(df, preprocess_features, i, int(round((i - 1)/2))))\n",
    "\n",
    "        print('# Diffs forward') \n",
    "        for i in diffs_space:\n",
    "            data_frames.append(_create_rolling_diff(df, preprocess_features, i))\n",
    "\n",
    "        print('# Diffs backward') \n",
    "        for i in diffs_space:\n",
    "            data_frames.append(_create_rolling_diff(df, preprocess_features, -i))\n",
    "\n",
    "        print('# Gradients forward')\n",
    "        for i in gradient_space:\n",
    "            data_frames.append(_create_gradient(df, preprocess_features, i, step=152))\n",
    "\n",
    "        print('# Gradients backward')\n",
    "        for i in gradient_space:\n",
    "            data_frames.append(_create_gradient(df, preprocess_features, -i, step=152))\n",
    "\n",
    "        print('# Merging')\n",
    "        df_merged = pd.concat(data_frames, axis=1)\n",
    "        \n",
    "        return df_merged\n",
    "    \n",
    "    \n",
    "    def _create_feature_space_(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        all_features = ['X_LOC', 'Y_LOC', 'Z_LOC', 'GROUP', 'FORMATION', 'Sector',\n",
    "                        'CALI', 'RSHA', 'RMED', 'RDEP', 'RHOB', 'GR', 'NPHI', 'PEF', \n",
    "                        'DTC', 'RXO', 'SP', 'BS', 'ROP', 'DTS', 'DCAL', 'DRHO', 'MUDWEIGHT',\n",
    "                        'RMIC', 'ROPA']\n",
    "        df = df[all_features].copy()\n",
    "        ftrs = self._make_features_(df)\n",
    "        df_fs = pd.concat([df, ftrs], axis=1)\n",
    "        print('# Done, created {} features'.format(df_fs.shape))\n",
    "        return  df_fs\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_models_(model_name: str) -> (list, list):\n",
    "        print('Load Models')\n",
    "        model_path = Path(model_name)\n",
    "        boost_models_pathes = sorted(model_path.glob('boost/*.pkl'))\n",
    "        categorizer_pathes = sorted(model_path.glob('categorizer/*.trfrm'))\n",
    "        boost_models = [joblib.load(boost_m) for boost_m in boost_models_pathes]\n",
    "        categorizer_models = [joblib.load(cat_m) for cat_m in categorizer_pathes]\n",
    "        \n",
    "        assert len(boost_models) == len(categorizer_models)\n",
    "        \n",
    "        return categorizer_models, boost_models \n",
    "\n",
    "        \n",
    "    def _apply_model_(self, X, categorizer, model):\n",
    "        X2 = categorizer.transform(X)\n",
    "        prediction = model.predict_proba(X2)\n",
    "        prediction = pd.DataFrame(prediction[:, :], index=X.index, columns=self.targets)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def _evaluate_model_(self, X, categorizers, models):\n",
    "        print('Appling Model')\n",
    "        preds = []\n",
    "        for categorizer, model in zip(categorizers, models):\n",
    "            prediction = self._apply_model_(X, categorizer, model)\n",
    "            preds.append(prediction)\n",
    "        return preds\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compine_prediction_(predictions: list) -> pd.DataFrame:\n",
    "        prediction = pd.concat(predictions, axis=1)\n",
    "        prediction2 = prediction.groupby(level=[0], axis=1).mean()\n",
    "#         prediction2 = reduce(lambda x, y: x + y, predictions)\n",
    "#         prediction2 = prediction2 / len(predictions)\n",
    "        return prediction2\n",
    "    \n",
    "    \n",
    "    def _preprocess(self, features: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Method to be run before inference. Contains things like\n",
    "        # stripping unwanted columns, replacing NaNs, and scaling \n",
    "        # or normalizing data\n",
    "        preimported_data = self._preprocess_raw_(features)\n",
    "        preprocessed_data = self._create_feature_space_(preimported_data)\n",
    "        return preprocessed_data\n",
    "        \n",
    "        \n",
    "    def predict(self, features: pd.DataFrame) -> np.ndarray:\n",
    "        # This function should be able to take in features in their\n",
    "        # raw, unprocessed form as read from the file test.csv and\n",
    "        # return predictions as an array integers of the same length\n",
    "        X = self._preprocess(features)\n",
    "        \n",
    "        model_name_1 = 'lgbm_model_1'\n",
    "        categorizers_1, models_1 = self._load_models_(model_name_1)\n",
    "        used_features_1 = np.load(f'{model_name_1}/features.npy', allow_pickle=True)\n",
    "        preds1 = self._evaluate_model_(X[used_features_1], categorizers_1, models_1)\n",
    "        p1 = self._compine_prediction_(preds1)\n",
    "\n",
    "        model_name_2 = 'lgbm_model_2'\n",
    "        categorizers_2, models_2 = self._load_models_(model_name_2)\n",
    "        used_features_2 = np.load(f'{model_name_2}/features.npy', allow_pickle=True)\n",
    "        preds2 = self._evaluate_model_(X[used_features_2], categorizers_2, models_2)\n",
    "        p2 = self._compine_prediction_(preds2)\n",
    "\n",
    "        P = (p1 + p2) / 2\n",
    "        p_final = self._postprocess_(P, features.copy())\n",
    "        return p_final\n",
    "    \n",
    "    @staticmethod\n",
    "    def _postprocess_(y_pred: pd.DataFrame, y_true: pd.DataFrame) -> np.ndarray:\n",
    "        inv_map = {4: 10, 8: 9}\n",
    "        y_pred = y_pred.rename(columns=inv_map)\n",
    "        \n",
    "        category_to_lithology = {0: 30000,\n",
    "                                 1: 65030,\n",
    "                                 2: 65000,\n",
    "                                 3: 80000,\n",
    "                                 4: 74000,\n",
    "                                 5: 70000,\n",
    "                                 6: 70032,\n",
    "                                 7: 88000,\n",
    "                                 8: 86000,\n",
    "                                 9: 99000,\n",
    "                                 10: 90000,\n",
    "                                 11: 93000}\n",
    "\n",
    "        y_pred = y_pred.idxmax(axis=1).map(category_to_lithology)\n",
    "        \n",
    "        y_true.rename(columns={'WELL':'Well', 'DEPTH_MD':'Depth'}, inplace=True)\n",
    "        y_true['Depth'] = (1000 * y_true['Depth']).round().astype(int)\n",
    "        y_true = y_true.set_index(['Well', 'Depth'])\n",
    "        \n",
    "        y_pred = y_pred.reindex(y_true.index)\n",
    "\n",
    "        return y_pred.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_raw = pd.read_csv('test.csv', sep=';')\n",
    "# data = Model().predict(test_raw)\n",
    "# pd.DataFrame(data=data).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
